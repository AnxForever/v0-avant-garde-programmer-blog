"use client"

import { useEffect, useRef, useState } from "react"
import Mic from "lucide-react/dist/esm/icons/mic"
import MicOff from "lucide-react/dist/esm/icons/mic-off"

export function AudioReactiveParticles() {
  const canvasRef = useRef<HTMLCanvasElement>(null)
  const [isListening, setIsListening] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const animationRef = useRef<number | null>(null)
  const streamRef = useRef<MediaStream | null>(null)

  useEffect(() => {
    const canvas = canvasRef.current
    if (!canvas) return

    const ctx = canvas.getContext("2d")
    if (!ctx) return

    canvas.width = canvas.offsetWidth
    canvas.height = canvas.offsetHeight

    const particles: Array<{
      x: number
      y: number
      vx: number
      vy: number
      radius: number
      color: string
    }> = []

    const colors = ["#FF00FF", "#00FF00", "#00FFFF", "#FFFF00", "#FF0000"]

    // åˆå§‹åŒ–ç²’å­
    for (let i = 0; i < 100; i++) {
      particles.push({
        x: Math.random() * canvas.width,
        y: Math.random() * canvas.height,
        vx: (Math.random() - 0.5) * 2,
        vy: (Math.random() - 0.5) * 2,
        radius: Math.random() * 3 + 1,
        color: colors[Math.floor(Math.random() * colors.length)],
      })
    }

    const animate = () => {
      ctx.fillStyle = "rgba(0, 0, 0, 0.1)"
      ctx.fillRect(0, 0, canvas.width, canvas.height)

      let audioLevel = 0

      // å¦‚æœæœ‰éŸ³é¢‘æ•°æ®ï¼Œè·å–éŸ³é‡
      if (analyserRef.current && isListening) {
        const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount)
        analyserRef.current.getByteFrequencyData(dataArray)
        audioLevel = dataArray.reduce((sum, val) => sum + val, 0) / dataArray.length / 255
      }

      particles.forEach((p) => {
        // æ ¹æ®éŸ³é¢‘çº§åˆ«è°ƒæ•´ç²’å­è¡Œä¸º
        const speedMultiplier = 1 + audioLevel * 3

        p.x += p.vx * speedMultiplier
        p.y += p.vy * speedMultiplier

        // è¾¹ç•Œæ£€æµ‹
        if (p.x < 0 || p.x > canvas.width) p.vx *= -1
        if (p.y < 0 || p.y > canvas.height) p.vy *= -1

        // ç»˜åˆ¶ç²’å­
        const size = p.radius * (1 + audioLevel * 2)
        ctx.beginPath()
        ctx.arc(p.x, p.y, size, 0, Math.PI * 2)
        ctx.fillStyle = p.color
        ctx.shadowBlur = 20
        ctx.shadowColor = p.color
        ctx.fill()
        ctx.shadowBlur = 0
      })

      animationRef.current = requestAnimationFrame(animate)
    }

    animate()

    return () => {
      if (animationRef.current) {
        cancelAnimationFrame(animationRef.current)
      }
      // æ¸…ç†éŸ³é¢‘èµ„æºï¼ˆç»„ä»¶å¸è½½æ—¶ï¼‰
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop())
      }
      if (audioContextRef.current) {
        audioContextRef.current.close()
      }
    }
  }, [isListening])

  const toggleAudio = async () => {
    if (!isListening) {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
        streamRef.current = stream
        const audioContext = new AudioContext()
        const analyser = audioContext.createAnalyser()
        const source = audioContext.createMediaStreamSource(stream)

        analyser.fftSize = 256
        source.connect(analyser)

        audioContextRef.current = audioContext
        analyserRef.current = analyser
        setIsListening(true)
        setError(null)
      } catch (err) {
        setError("æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æµè§ˆå™¨æƒé™")
        console.error("Microphone access error:", err)
      }
    } else {
      // åœæ­¢æ‰€æœ‰ MediaStream tracksï¼ˆå…³é—­éº¦å…‹é£æŒ‡ç¤ºç¯ï¼‰
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop())
        streamRef.current = null
      }
      if (audioContextRef.current) {
        audioContextRef.current.close()
        audioContextRef.current = null
        analyserRef.current = null
      }
      setIsListening(false)
    }
  }

  return (
    <div className="relative w-full h-full bg-black">
      <canvas ref={canvasRef} className="w-full h-full" />

      <div className="absolute bottom-6 left-6 right-6 bg-black/80 backdrop-blur-sm border-4 border-white p-6 shadow-[8px_8px_0px_0px_rgba(255,0,255,1)]">
        <div className="flex justify-between items-center">
          <div>
            <h3 className="font-black text-xl mb-2">éŸ³é¢‘ååº”ç²’å­ç³»ç»Ÿ</h3>
            <p className="font-mono text-sm text-gray-400">
              {isListening ? "ğŸ¤ æ­£åœ¨ç›‘å¬éº¦å…‹é£..." : "ç‚¹å‡»æŒ‰é’®å¼€å§‹æ•æ‰å£°éŸ³"}
            </p>
            {error && <p className="font-mono text-sm text-accent-red mt-2">{error}</p>}
          </div>
          <button
            onClick={toggleAudio}
            className={`w-16 h-16 border-4 border-white flex items-center justify-center transition-all shadow-[4px_4px_0px_0px_rgba(255,255,255,1)] hover:shadow-[8px_8px_0px_0px_rgba(255,255,255,1)] hover:translate-x-[-4px] hover:translate-y-[-4px] ${
              isListening ? "bg-accent-green" : "bg-accent-pink"
            }`}
          >
            {isListening ? <MicOff className="w-8 h-8 text-black" /> : <Mic className="w-8 h-8 text-black" />}
          </button>
        </div>
      </div>
    </div>
  )
}
